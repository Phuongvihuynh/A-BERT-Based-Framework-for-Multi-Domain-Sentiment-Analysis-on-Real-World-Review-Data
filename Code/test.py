# -*- coding: utf-8 -*-
"""Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cHhZS339cdVllWUtBRt9L-90r0qyEU1b
"""

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from torch.nn.utils.rnn import pad_sequence
from sklearn.metrics import classification_report

# 1. Load tokenizer
model_name = "prajjwal1/bert-mini"  # hoặc "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

# 2. Hàm tokenize
def tokenize_data(df, tokenizer, max_length=128):
    encodings = tokenizer(
        df['sentence'].tolist(),
        truncation=True,
        padding='max_length',
        max_length=max_length
    )
    df = df.copy()
    df['input_ids'] = pd.Series(encodings['input_ids'])
    df['attention_mask'] = pd.Series(encodings['attention_mask'])
    df['sentiment_labels'] = df['sentiment_label'].astype(int)
    return df

# 3. Dataset class
class AmazonSentimentDataset(Dataset):
    def __init__(self, df):
        self.input_ids = df['input_ids'].tolist()
        self.attention_masks = df['attention_mask'].tolist()
        self.labels = df['sentiment_labels'].tolist()

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return (torch.tensor(self.input_ids[idx], dtype=torch.long),
                torch.tensor(self.attention_masks[idx], dtype=torch.long),
                torch.tensor(self.labels[idx], dtype=torch.long))

# 4. Collate
def collate_fn(batch):
    input_ids, attention_masks, labels = zip(*batch)
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)
    labels = torch.stack(labels)
    token_type_ids = torch.zeros_like(input_ids)
    return input_ids, token_type_ids, attention_masks, labels

# 5. Kiến trúc mô hình giống như lúc train
class ABSABert(torch.nn.Module):
    def __init__(self, model_name):
        super(ABSABert, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = torch.nn.Dropout(0.3)
        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 3)
        self.loss_fn = torch.nn.CrossEntropyLoss()  # thêm dòng này để khớp


    def forward(self, input_ids, token_type_ids, attention_mask):
        out = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True
        )
        pooled_output = self.dropout(out.pooler_output)
        logits = self.linear(pooled_output)
        return logits

# 6. Load mô hình đã huấn luyện
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ABSABert(model_name).to(device)
state_dict = torch.load('/content/prajjwal1_bert-mini_BEST_20_amazon.pt', map_location=device)
# Lọc bỏ key 'loss_fn.weight'
filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('loss_fn')}
model.load_state_dict(filtered_state_dict, strict=False)
model.eval()

# 7. Load và chuẩn hóa test data
test_data = pd.read_csv('/content/test_data.csv')  # hoặc đường dẫn khác
test_data = test_data.dropna(subset=['sentence', 'sentiment_label'])
test_df = tokenize_data(test_data, tokenizer)
test_ds = AmazonSentimentDataset(test_df)
test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, collate_fn=collate_fn)

# 8. Đánh giá mô hình
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, token_type_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, token_type_ids, attention_mask)
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# 9. In báo cáo
print(classification_report(all_labels, all_preds, target_names=["negative", "neutral", "positive"]))