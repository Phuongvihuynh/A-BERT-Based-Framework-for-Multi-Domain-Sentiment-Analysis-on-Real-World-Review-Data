# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQF8MP42_aXRDumX8IGuflSEorG1b_00
"""

!pip install pandas transformers scikit-learn

import os
import random
import numpy as np
import pandas as pd
import time
import csv
from datetime import datetime

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from transformers import BertTokenizer, BertModel
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import StratifiedKFold

# ---------------------- Set Seed ----------------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# ---------------------- Parameters ----------------------
batch_size = 520
learning_rate = 1e-5
num_epochs = 50
patience = 10
model_name = "prajjwal1/bert-mini"
save_model_dir = "./working"
os.makedirs(save_model_dir, exist_ok=True)

log_path = os.path.join(save_model_dir, "training_log.csv")
with open(log_path, mode='w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["timestamp", "fold", "epoch", "train_loss", "train_acc", "val_loss", "val_acc", "epoch_time_sec"])

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tokenizer = BertTokenizer.from_pretrained(model_name)

# ---------------------- Load Your DataFrame ----------------------
k_fold_merge_train_valid = pd.read_csv("k_fold_merge_train_valid.csv")
k_fold_merge_train_valid = k_fold_merge_train_valid.dropna(subset=['sentence', 'sentiment_label'])
k_fold_merge_train_valid = k_fold_merge_train_valid.reset_index(drop=True)

# ---------------------- Stratified K-Fold ----------------------
skf = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)
k_fold_merge_train_valid['fold'] = -1
for fold, (_, val_idx) in enumerate(skf.split(k_fold_merge_train_valid, k_fold_merge_train_valid['sentiment_label'])):
    k_fold_merge_train_valid.loc[val_idx, 'fold'] = fold

# ---------------------- Tokenization Function ----------------------
def tokenize_data(df, tokenizer, max_length=128):
    encodings = tokenizer(
        df['sentence'].tolist(),
        truncation=True,
        padding='max_length',
        max_length=max_length
    )
    df = df.copy()
    df['input_ids'] = encodings['input_ids']
    df['attention_mask'] = encodings['attention_mask']
    df['sentiment_labels'] = df['sentiment_label'].astype(int)
    return df

# ---------------------- Dataset ----------------------
class AmazonSentimentDataset(Dataset):
    def __init__(self, df):
        self.input_ids = df['input_ids'].tolist()
        self.attention_masks = df['attention_mask'].tolist()
        self.labels = df['sentiment_labels'].tolist()

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return (torch.tensor(self.input_ids[idx], dtype=torch.long),
                torch.tensor(self.attention_masks[idx], dtype=torch.long),
                torch.tensor(self.labels[idx], dtype=torch.long))

def collate_fn(batch):
    input_ids, attention_masks, labels = zip(*batch)
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)
    labels = torch.stack(labels)
    token_type_ids = torch.zeros_like(input_ids)
    return input_ids, token_type_ids, attention_masks, labels

# ---------------------- Model ----------------------
class ABSABert(torch.nn.Module):
    def __init__(self, model_name, class_weights_tensor):
        super(ABSABert, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = torch.nn.Dropout(0.3)
        self.linear = torch.nn.Linear(self.bert.config.hidden_size, 3)
        self.loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)

    def forward(self, input_ids, token_type_ids, attention_mask, labels=None):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids, return_dict=True)
        pooled_output = self.dropout(out.pooler_output)
        logits = self.linear(pooled_output)

        if labels is not None:
            loss = self.loss_fn(logits, labels)
            return loss, logits
        return logits

# ---------------------- Train/Evaluate ----------------------
def train_epoch(model, optimizer, dataloader, device, clip_value=1.0):
    model.train()
    total_loss, correct, total = 0, 0, 0
    start_time = time.time()
    step_count = 0

    for batch in dataloader:
        step_count += 1
        input_ids, token_type_ids, attention_mask, labels = [b.to(device) for b in batch]
        optimizer.zero_grad()
        loss, outputs = model(input_ids, token_type_ids, attention_mask, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)
        optimizer.step()
        total_loss += loss.item()
        _, preds = torch.max(outputs, dim=1)
        correct += torch.sum(preds == labels).item()
        total += len(labels)

    end_time = time.time()
    avg_time_per_step = (end_time - start_time) / step_count
    print(f"\u23F1\ufe0f Thời gian trung bìi gian trung b\xecnh mỗi bước huấn luyện: {avg_time_per_step:.2f} giân: {avg_time_per_step:.2f} gi\xe2y")

    return total_loss / len(dataloader), correct / total

def evaluate_epoch(model, dataloader, device):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for batch in dataloader:
            input_ids, token_type_ids, attention_mask, labels = [b.to(device) for b in batch]
            loss, outputs = model(input_ids, token_type_ids, attention_mask, labels)
            total_loss += loss.item()
            _, preds = torch.max(outputs, dim=1)
            correct += torch.sum(preds == labels).item()
            total += len(labels)
    return total_loss / len(dataloader), correct / total

# def train_model(model, optimizer, train_loader, val_loader, num_epochs, device, patience=2, fold=0, log_path=None):
#     best_val_loss = float('inf')
#     patience_counter = 0
#     for epoch in range(1, num_epochs + 1):
#         start_time_epoch = time.time()

#         train_loss, train_acc = train_epoch(model, optimizer, train_loader, device)
#         val_loss, val_acc = evaluate_epoch(model, val_loader, device)

#         end_time_epoch = time.time()
#         epoch_time = end_time_epoch - start_time_epoch

#         print(f"Epoch {epoch}:")
#         print(f"  Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}")
#         print(f"  Val   Loss = {val_loss:.4f}, Val   Acc = {val_acc:.4f}")

#         if log_path is not None:
#             with open(log_path, mode='a', newline='') as f:
#                 writer = csv.writer(f)
#                 writer.writerow([
#                     datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
#                     fold, epoch,
#                     f"{train_loss:.4f}", f"{train_acc:.4f}",
#                     f"{val_loss:.4f}", f"{val_acc:.4f}",
#                     f"{epoch_time:.2f}"
#                 ])

#         if val_loss < best_val_loss:
#             best_val_loss = val_loss
#             best_model_state = model.state_dict()
#             patience_counter = 0
#         else:
#             patience_counter += 1
#             if patience_counter >= patience:
#                 print("Early stopping triggered.")
#                 break
#     return best_model_state, best_val_loss, val_acc
def train_model(model, optimizer, train_loader, val_loader, num_epochs, device, patience=10, fold=0):
    best_val_loss = float('inf')
    best_model_state = None
    recent_val_losses = []
    log_rows = []

    for epoch in range(1, num_epochs + 1):
        start_time_epoch = time.time()

        train_loss, train_acc = train_epoch(model, optimizer, train_loader, device)
        val_loss, val_acc = evaluate_epoch(model, val_loader, device)

        end_time_epoch = time.time()
        epoch_time = end_time_epoch - start_time_epoch

        print(f"Epoch {epoch}:")
        print(f"  Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}")
        print(f"  Val   Loss = {val_loss:.4f}, Val   Acc = {val_acc:.4f}")

        # Lưu log vào bộ nhớ tạm
        log_rows.append([
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            fold, epoch,
            f"{train_loss:.4f}", f"{train_acc:.4f}",
            f"{val_loss:.4f}", f"{val_acc:.4f}",
            f"{epoch_time:.2f}"
        ])

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict()
            recent_val_losses = []
        else:
            recent_val_losses.append(val_loss)
            if len(recent_val_losses) >= patience:
                print(f"Early stopping triggered after {patience} epochs with no improvement.")
                break

    return best_model_state, best_val_loss, val_acc, log_rows


# ---------------------- Run All Folds & Save Best ----------------------
best_val_acc = 0.0
best_model_state = None
best_log_rows = []  # lưu lại log tốt nhất

start_time_all = time.time()

for fold in range(20):
    print(f"\nFold {fold}")
    train_df = k_fold_merge_train_valid[k_fold_merge_train_valid['fold'] != fold]
    val_df = k_fold_merge_train_valid[k_fold_merge_train_valid['fold'] == fold]

    train_df = tokenize_data(train_df, tokenizer)
    val_df = tokenize_data(val_df, tokenizer)

    y_train = train_df['sentiment_labels'].tolist()
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)

    train_ds = AmazonSentimentDataset(train_df)
    val_ds = AmazonSentimentDataset(val_df)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

    model = ABSABert(model_name, class_weights_tensor).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

    model_state, val_loss, val_acc, log_rows = train_model(
        model, optimizer, train_loader, val_loader, num_epochs, device,
        patience=patience, fold=fold
    )

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_model_state = model_state
        best_log_rows = log_rows  # lưu log cho fold tốt nhất
        print(f"New best model at Fold {fold} with Acc = {val_acc:.4f}")

end_time_all = time.time()
total_minutes = (end_time_all - start_time_all) / 60
print(f"\n✅ Tổng thời gian huấn luyện toàn bộ folds: {total_minutes:.2f} phút")

# ---------------------- Save Final Best Model ----------------------
final_model_path = os.path.join(save_model_dir, f"{model_name.replace('/', '_')}_BEST_20.pt")
torch.save(best_model_state, final_model_path)
print(f"Best model saved to: {final_model_path} (Val Acc = {best_val_acc:.4f})")

# ---------------------- Save Best Fold's Training Log ----------------------
log_path = os.path.join(save_model_dir, "training_log.csv")
with open(log_path, mode='w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["timestamp", "fold", "epoch", "train_loss", "train_acc", "val_loss", "val_acc", "epoch_time_sec"])
    writer.writerows(best_log_rows)

print(f"Training log for best fold saved to: {log_path}")